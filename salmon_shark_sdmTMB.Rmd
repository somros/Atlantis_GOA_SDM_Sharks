---
title: "Atlantis biomass distributions with sdmTMB - lat, lon"
author: "Alberto Rovellini"
date: "October 25 2021"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: true
---

This is a template for fitting sdmTMB to observed data on salmon sharks (pelagic trawl gear). Starting with a version with lat, lon, and distance from shore, i.e. ignoring the temporal component in the data because of data sparsity. 

This workflow is based on the following assumptions:

1. We use lat, lon and distance from shore as predictors. 
2. We predict over a regular grid. The size of this grid is 10 km at the moment for computational efficiency, but this is arbitrary and we may need to test different grid sizes and see how the results change. This is the grid size we are using for the GOA, but here we are using the same SPDE mesh for a much smaller area, and therefore we might need to adjust the prediction grid accordingly.
3. We are not so interested in accurate predictions for any one year, but rather in representative means of where the sharks have been over the last few decades. Here, we run the model without a temporal component. 

**IMPORTANT**: model performance metrics and in general model predictions do not look good for this data. Note that this is based on fishery data, so the spatial stratification of the samples follows the fishing boats rather than any sampling design (and it is not random). There are few data points for a very large area, distance from shore may not be a great predictor (in general,and especially in places like BC, we need to think about this for everything that uses it - best should be a combination of distance from shore and depth, but the GOA is large and this may or may not help), and this species is highly mobile anyway. I would probably stay away from this and just use heuristic general distributions based on tagging studies.

```{r}
library(sdmTMB)
library(tidyverse)
library(sf)
library(maps)
library(mapdata)
library(rbgm)
library(viridis)
library(kableExtra)
```

```{r}
select <- dplyr::select
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Read data
```{r}
ss_data <- readRDS('../data/Fishery/salmon_shark.RDS')

atlantis_bgm <- read_bgm('../data/GOA_WGS84_V4_final.bgm')
atlantis_box <- atlantis_bgm %>% box_sf()
# utilities
atlantis_crs <- atlantis_bgm$extra$projection
atlantis_bbox <- atlantis_box %>% st_bbox()

# coast mask
coast <- map("worldHires", regions = c("Canada", "USA"), plot = FALSE, fill = TRUE)
coast_sf <- coast %>% st_as_sf() %>% st_transform(crs = atlantis_crs) %>% st_combine()
```

Transform the salmon shark observer data to the Atlantis CRS, and divide by 1000 for numerical stability.
```{r}
ss_data <- ss_data %>% 
  st_transform(crs=atlantis_crs) %>% 
  mutate(lon = st_coordinates(.)[,1]/1000,
         lat = st_coordinates(.)[,2]/1000) %>%
  mutate(distance = as.vector(st_distance(geometry,coast_sf))/1000) %>%
  st_set_geometry(NULL)

# a number of hauls are missing the duration in minutes, even several that have some salmon shark catches. That is unfortunate, but we need to discard those points because the duration is the only indication of effort that we have.
ss_data <- ss_data %>% filter(!is.na(Duration..Min.) & Duration..Min.>0)
```

Take a quick look at the data spatially.
```{r}
ggplot()+
  geom_point(data = ss_data, aes(lon*1000, lat*1000, colour = CPUE_weight), size = 1.5)+
  scale_colour_viridis_c()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim=c(atlantis_bbox$xmin, atlantis_bbox$xmax),ylim=c(atlantis_bbox$ymin, atlantis_bbox$ymax))+
  theme_minimal()+
  #facet_wrap(~Year)+
  labs(title = 'Salmon shark catches from NORPAC Observer data (pelagic trawl gear)')
```
Big data gap in the southeast and BC.

Take a quick look at time series of total density from survey data. 
```{r, fig.width = 6, fig.height = 4}
cpue_year <- ss_data %>% group_by(Year) %>% summarise(CPUE = sum(CPUE_weight, na.rm = TRUE))

ggplot(cpue_year, aes(Year, log(CPUE)))+
  geom_point()+
  geom_path()+
  theme_minimal()+
  labs(title = 'Salmon shark catches from NORPAC Observer data (pelagic trawl gear)')
```

# sdmTMB

## Create spatial mesh

This is the mesh that the sdmTMB algorithm uses to estimate spatial autocorrelation. The speed of model running is highly dependent on number of knots. 100 is quite low, you'd want to make sure it was robust by checking multiple resolutions when you have a model you want to actually use (people use like 350-450 or something).

**Note:** SPDE = Stochastic Partial Differential Equations approach. Some material can be found [here](https://becarioprecario.bitbucket.io/spde-gitbook/ch-intro.html#sec:spde), but basically it is a way of calculating the position of the mesh knots. 
```{r}
ss_spde <- make_mesh(ss_data, c("lon", "lat"), cutoff = 20) # how do we know that this is big enough?
plot(ss_spde)
```

Check out the distribution of the biomass density response variable.
```{r, fig.width = 6, fig.height = 4}
hist(ss_data$CPUE_weight, breaks = 30)
```

We may have some big outliers that we potentially may need to get rid of (apparently some tows fished over 20 t of salmon sharks, how does that even happen with a mostly solitary pelagic shark? It does seem to happen often according to the observer data).

```{r, fig.width = 6, fig.height = 4}
hist(log1p(ss_data$CPUE_weight), breaks = 30)
```

Proportion of zeroes in percentage.
```{r}
length(which(ss_data$CPUE_weight == 0))/nrow(ss_data)*100
```
Mostly absent from tows.

## Space, time, and distance from coast model.

Model type: the distribution of the response variable plotted above should give a sense of what model is most appropriate. CPUE data for many of these species resemble a Tweedie distribution when log-transformed, so we use a Tweedie model with a log link. Some groups may warrant a different model, and this will be evaluated case-by-case depending on convergence issues, distribution of model residuals, and model skill metrics (see below).

Distance from shore is in km here. We use a spline with 5 knots like in the depth models to start, but we will want to test this.

We run the model with year factor at first.
```{r, results = FALSE}
m_dist <- sdmTMB(
  data = ss_data, 
  formula = CPUE_weight ~ 0 + s(distance, k = 5), #+ as.factor(year), 
  time = NULL,#"year", 
  spde = ss_spde, 
  reml = TRUE,
  anisotropy = FALSE,
  spatial_trend = FALSE, 
  spatial_only = FALSE,
  silent = FALSE,
  control = sdmTMBcontrol(),
  family = tweedie(link = "log"))
```

Rerun with extra optimization steps in case of gradient > 0.001.
```{r, results = FALSE}
if(abs(max(m_dist$gradients))>0.001){
  m_dist <- sdmTMB(
  data = ss_data, 
  formula = CPUE_weight ~ 0 + s(distance, k = 5), #+ as.factor(year), 
  time = NULL,#"year", 
  spde = ss_spde, 
  reml = TRUE,
  anisotropy = FALSE,
  spatial_trend = FALSE, 
  spatial_only = FALSE,
  silent = FALSE,
  control = sdmTMBcontrol(),
  family = tweedie(link = "log"))
}
```

Check information on model convergence. From the nlminb help page we know that an integer 0 indicates succesful convergence. Additional information on convergence can be checked with m_depth$model$message. According to the original PORT optimization documentation, “Desirable return codes are 3, 4, 5, and sometimes 6”.
```{r}
if(m_dist$model$convergence == 0){print("The model converged.")} else {print("Check convergence issue.")}
m_dist$model$message
```

```{r}
max(m_dist$gradients)
```

```{r}
m_dist$tmb_obj$report()$range
```

Check out model residuals.
```{r, fig.width = 6, fig.height = 4}
ss_data$resids <- residuals(m_dist) # randomized quantile residuals
hist(ss_data$resids)
```

And QQ plot.
```{r}
qqnorm(ss_data$resids)
abline(a = 0, b = 1)
```
Heavy skew for 'Diving_Fish', for example. Bad fit for high densities?

Plot the response curve from the depth smooth term.
```{r}
plot(m_dist$mgcv_mod, rug = TRUE)
```

Finally, plot the residuals in space. If residuals are constantly larger/smaller in some of the areas, it may be sign that the model is biased and it over/underpredicts consistently for some areas. Residuals should be randomly distributed in space. 

```{r, fig.width = 12, fig.height=6}
ss_sf <- ss_data %>% mutate(lon=lon*1000,lat=lat*1000) %>% st_as_sf(coords = c(x = "lon", y = "lat"), crs = atlantis_crs) # turn to spatial object

#define coordinate limits for BGM projection
coord_lims <- ss_sf %>% st_coordinates() %>% data.frame() %>% set_names(c("x","y")) %>% summarise(xmin=min(x),xmax=max(x),ymin=min(y),ymax=max(y))

ggplot()+
  geom_sf(data = ss_sf, aes(color = resids, alpha = .8))+
  scale_color_viridis()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))+
  theme_minimal()+
  labs(title = "Model residuals in space")#+
  #facet_wrap(~year, ncol = 2)
```

# Predictions from SDM

Take a grid (which must contain information on the predictors we used to build the model) and predict the biomass index over such grid based on the predictors. The grid is currently a regular grid with 10-km cell size, but 10 km might not be enough to get prediction points in all boxes - especially for a couple very small and narrow boxes at the western end of the model domain. Revisit this if necessary, but a finer mesh could be difficult to justify compared to the density of the survey data. The grid covers the entire Atlantis model domain, including the non-dynamic boundary boxes (deeper than 1000 m).

Read in the Atlantis prediction grid (10 km) modified in Atlantis_grid_covars.R (code not included here).

**For NORPAC sharks:** This step will be different:
- Read Atlantis grid.
- Ditch depth.
- Turn it to sf.
- Calculate each point's distance from shore.
- Divide coords by 1000.
- Use dist and linear coords as predictors.

```{r}
load("../data/atlantis_grid_depth.Rdata")

atlantis_grid <- atlantis_grid_depth %>% 
  select(-depth,-insideY,-insideX) %>% 
  st_as_sf(coords=c("x","y"), crs = atlantis_crs) %>%
  mutate(distance = as.vector(st_distance(geometry,coast_sf))/1000) %>%
  mutate(lon=st_coordinates(geometry)[,1]/1000,lat=st_coordinates(geometry)[,2]/1000) %>%
  st_set_geometry(NULL)

# crop the prediction grid to the data extent, the predictions are way off for BC for some reason (i.e. very high abundance is predicted around Haida Gwaii, it is possible that it picks up the latitude and proximity to the coastline of some of the large outliers?).

atlantis_grid <- atlantis_grid %>% filter(lon<max(ss_data$lon))

#atlantis_grid %>% ggplot() + geom_point(aes(x=lon,y=lat))
```

```{r}
# add year column
# all_years <- levels(factor(goaierp_data$year))
# 
# atlantis_grid <- atlantis_grid_dist[rep(1:nrow(atlantis_grid_dist), length(all_years)),]
# atlantis_grid$year <- as.numeric(rep(all_years, each = nrow(atlantis_grid_depth)))
```

Make SDM predictions onto new data from depth model. **Back-transforming here, is this sensible?**
```{r}
predictions_ss <- predict(m_dist, newdata = atlantis_grid, return_tmb_object = TRUE)
atlantis_grid$estimates <- exp(predictions_ss$data$est) #Back-transforming here, is this sensible?

atlantis_grid_sf <- atlantis_grid %>% mutate(lon=lon*1000,lat=lat*1000) %>% st_as_sf(coords = c("lon", "lat"), crs = atlantis_crs) # better for plots
coord_lims <- atlantis_grid_sf %>% st_bbox()
```

Plotting Canada as well here, only because if we leave that out we need to leave out the AI as well. It will be best to replace Canada predictions with Canada data.
```{r}
ggplot()+
  geom_sf(data = atlantis_grid_sf, aes(color=log1p(estimates)), size = 2)+ # taking the log for visualisation
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))+
  scale_color_viridis(name = 'CPUE (extrapolated kg / min)')+
  theme_minimal()+
  labs(title = 'Predicted CPUE for salmon shark')
```

Attribute the predictions to their respective Atlantis box, so that we can take box averages.
```{r}
atlantis_grid_means <- atlantis_grid %>% group_by(box_id) %>%
  summarise(mean_estimates = mean(estimates, na.rm = TRUE)) %>% ungroup() 

# join this with the box_sf file

predictions_by_box <- atlantis_box %>% inner_join(atlantis_grid_means, by = "box_id")
```

See estimates per box for all years combined. Silence boundary boxes as they throw the scale out of whack (and they do not need predictions). 
```{r, fig.width = 12, fig.height = 5.5}
predictions_by_box <- predictions_by_box %>% rowwise() %>% mutate(mean_estimates = ifelse(isTRUE(boundary), NA, mean_estimates))

ggplot()+
  geom_sf(data = predictions_by_box, aes(fill = log1p(mean_estimates)))+ # taking the log for visualisation
  scale_fill_viridis(name = 'Log CPUE')+
  theme_minimal()+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))+  
  labs(title = '')
```
Somehow they are grossly overpredicted in the SE, where there is only one data point of 0 CPUE. Why does that happen? Does it pick up the latitude being the same as some tows around Kodiak?

Plot the raw data again for comparison.
```{r, fig.width = 12, fig.height = 5.5}
ss_data %>% mutate(lon=lon*1000,lat=lat*1000) %>% st_as_sf(coords = c("lon","lat"), crs=atlantis_crs) %>% 
  filter(CPUE_weight>0) %>% # just for visualisation
  ggplot()+
  geom_sf(aes(colour = log1p(CPUE_weight)), size = 2, alpha = .5)+ # taking the log for visualisation
  scale_colour_viridis_c(name = 'Log CPUE')+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(ss_data$lon*1000),max(ss_data$lon*1000)), ylim=c(min(ss_data$lat*1000),max(ss_data$lat*1000)))+  theme_minimal()+
  #facet_wrap(~year, ncol = 2)+
  labs(title = '')
```

Have a look at density by distance from shore. 
```{r, fig.width = 6, fig.height = 4}
ggplot(data = ss_data, aes(x = distance, y = log1p(CPUE_weight)))+#, color = log1p(num_km2)))+
  scale_color_viridis()+
  geom_point()+
  theme_minimal()+
  labs(title = "CPUEy and distance from shore")
```

Plot data and predictions distributions. These are the data.
```{r, fig.width = 6, fig.height = 4}
ggplot(data = ss_data, aes(log1p(CPUE_weight)))+
  geom_histogram(colour = "black", fill = 'grey80')+
  theme_minimal()
```

And these are the predictions over the 10 km grid.
```{r, fig.width = 6, fig.height = 4}
ggplot(data = atlantis_grid, aes(log1p(estimates)))+
  geom_histogram(colour = "black", fill = 'grey80')+
  theme_minimal()
```

Let’s have a look at the variance per box over all years. We use the coefficient of variation, because CPUE varies widely between boxes.

```{r}
atlantis_grid_cv <- atlantis_grid %>% group_by(box_id) %>% summarise(cv = sd(estimates)/mean(estimates)) %>% ungroup()

cv_by_box <- atlantis_box %>% inner_join(atlantis_grid_cv, by = "box_id")
```

```{r}
ggplot()+
  geom_sf(data = cv_by_box[-which(cv_by_box$boundary),], aes(fill = cv))+
  scale_fill_viridis(name = "CV of density")+
  geom_sf(data = coast_sf, colour = "black", fill = "grey80")+
  theme_minimal()+
  coord_sf(xlim = c(coord_lims$xmin,coord_lims$xmax), ylim=c(coord_lims$ymin,coord_lims$ymax))
```

# Model skill

Trying to evaluate model skill by having a look at how well model predictions align with observations.

Since this is a spatially-explicit approach, we need observations and predictions at the same location. We use the locations of all NPPSD as a prediction grid.
```{r}
#make a prediction grid from the nppsd data itself
ss_grid <- ss_data %>% dplyr::select(lon, lat, distance) 

# predict on this grid
predictions_at_locations <- predict(m_dist, newdata = ss_grid, return_tmb_object = TRUE)
ss_grid$predictions <- exp(predictions_at_locations$data$est) # back-transforming here
```

Now join by year and coordinates to have predictions at the sampling points.
```{r, fig.width = 12, fig.height = 6}
ss_corr <- ss_data %>% mutate(pred_at_obs=ss_grid$predictions)
```

## Observed versus predicted

```{r}
paste0("Pearson's coef observations vs predictions: ", cor(ss_corr$CPUE_weight, ss_corr$pred_at_obs, use = "everything", method = "pearson"))
```
Real low.?

Plot.
```{r}
ggplot(ss_corr, aes(x = log1p(CPUE_weight), y = log1p(pred_at_obs)))+ # log for visualisation
  geom_point(aes(color = distance))+
  scale_color_viridis()+
  geom_abline(intercept = 0, slope = 1)+
  theme_minimal()
```
Correlations is really low.

Plot zero catch from the data and the relative predictions. Turn to sf for plotting.

```{r}
ss_corr %>% filter(CPUE_weight == 0) %>%
  mutate(lon=lon*1000,lat=lat*1000) %>%
  st_as_sf(coords = c(x = "lon", y = "lat"), crs = atlantis_crs) %>%
  ggplot()+
  geom_sf(aes(color = log1p(pred_at_obs)))+
  geom_sf(data = coast_sf)+
  coord_sf(xlim = c(min(ss_data$lon*1000),max(ss_data$lon*1000)), ylim=c(min(ss_data$lat*1000),max(ss_data$lat*1000)))+  theme_minimal()+
  scale_color_viridis()+
  theme_minimal()+
  labs(title = "Model predictions at zero-density locations")#+
  #facet_wrap(~year, ncol = 2)
```

What about the relationship between model residuals and distance from shore?
```{r}
ss_data %>%
  ggplot()+
  geom_point(aes(x = distance, y = resids, color = log1p(CPUE_weight)))+
  geom_hline(yintercept = 0, color = "red", linetype = "dashed")+
  scale_color_viridis()+
  theme_minimal()
```

## Root Mean Square Error (RMSE)

Calculate RMSE between predicted and observed values.
```{r}
paste("RMSE:", sqrt(sum((ss_corr$pred_at_obs - ss_corr$CPUE_weight)^2)/nrow(ss_corr)), " extrapolated kg / min", sep = " ") ### traditional rmse metric, in units kg km2
```

Normalised RMSE. 
```{r}
rmse_cv <- sqrt(sum((ss_corr$pred_at_obs - ss_corr$CPUE_weight)^2)/nrow(ss_corr))/(max(ss_corr$CPUE_weight)-min(ss_corr$CPUE_weight))*100 #### normalised rmse, expressed as a % of the range of observed biomass values, sort of approximates a coefficient of variation 
paste("Normalised RMSE:", paste0(rmse_cv, "%"), sep = " ")
```
What is a good value here?

# Total biomass and biomass per box

The current estimated CPUE is in kg km$^{-2}$. So, just I just turn that into biomss per box. Remember that the area is in m$^2$ for the boxes, so need to divide by 1,000,000.
```{r}
predictions_by_box <- predictions_by_box %>% mutate(abundance = mean_estimates*area*1e-06)

predictions_by_box %>% select(box_id, mean_estimates, abundance) %>% st_set_geometry(NULL) %>% kable(align = 'lccc', format = "markdown", 
      col.names = c("Box", "CPUE_weightity (num km-2)", "Abundance"))
```

Write out a .csv.
```{r}
out <- predictions_by_box %>% st_set_geometry(NULL)

#write.csv(x = out, file = paste0("../outputs/",ss_data$name[1],"_ss.csv"), row.names = FALSE)
```

# Validation metrics

Let’s produce a table that includes: convergence metrics; Pearson’s correlation coefficient for predicted vs observed; RMSE; and normalised RMSE.
```{r}
# val <- data.frame(ss_data$name[1], # group
#                   m_dist$model$convergence, # convergence
#                   m_dist$model$message, # more convergence
#                   max(m_dist$gradients), # max gradient
#                   m_dist$tmb_obj$report()$range, # practical range
#                   cor(ss_corr$CPUE_weight, ss_corr$pred_at_obs, use = "everything", method = "pearson"), # correlation
#                   sqrt(sum((ss_corr$pred_at_obs - ss_corr$CPUE_weight)^2)/nrow(ss_corr)),# RMSE
#                   sqrt(sum((ss_corr$pred_at_obs - ss_corr$CPUE_weight)^2)/nrow(ss_corr))/(max(ss_corr$CPUE_weight)-min(ss_corr$CPUE_weight))*100 # NRMSE
# ) %>% set_names(c("Group","Convergence","Message","Max gradient","Practical range (km)","Pearson's correlation","RMSE","NRMSE(%)"))
# 
# val
```

```{r}
#write.csv(x = val, file = paste0("../outputs/","validation_",ss_data$name[1],"_ss.csv"), row.names = FALSE)
```

